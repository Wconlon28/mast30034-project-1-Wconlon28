{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/19 19:53:27 WARN Utils: Your hostname, MIchaels-HP resolves to a loopback address: 127.0.1.1; using 172.20.216.58 instead (on interface eth0)\n",
      "23/08/19 19:53:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/19 19:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/19 19:53:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/19 19:53:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session \n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Yellow Preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather initial information about our 2022 TLC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_sdf = spark.read.parquet('../data/raw/yellow_tlc_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips in the original data is: 19838515\n"
     ]
    }
   ],
   "source": [
    "yellow_count_pre = yellow_sdf.count()\n",
    "print('Number of trips in the original data is:', yellow_count_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we want to ensure everything has consistent casing \n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in yellow_sdf.columns]\n",
    "yellow_sdf = yellow_sdf.select(*consistent_col_casing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove features from our dataframe that will have no use later in modelling or analysis\n",
    "yellow_sdf = yellow_sdf.drop(\"store_and_fwd_flag\", \"vendorid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "| 2001-01-01 00:03:14|  2001-01-01 01:12:47|            1.0|        20.77|       2.0|         132|         230|           2|       52.0|  0.0|    0.5|       0.0|        6.55|                  0.3|       61.85|                 2.5|        0.0|\n",
      "| 2001-01-01 00:27:45|  2001-01-01 00:34:17|            1.0|         0.88|       1.0|          48|         143|           2|        6.0|  1.0|    0.5|       0.0|         0.0|                  0.3|        10.3|                 2.5|        0.0|\n",
      "| 2001-01-01 01:02:18|  2001-01-01 17:20:19|            1.0|          3.1|       1.0|         237|         234|           1|       14.0|  1.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|        0.0|\n",
      "| 2001-01-01 01:23:51|  2001-01-01 01:50:08|            1.0|         4.68|       1.0|         230|         231|           2|       20.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        24.8|                 2.5|        0.0|\n",
      "| 2001-01-01 01:52:48|  2001-01-01 17:37:26|            1.0|         6.95|       1.0|         231|         264|           1|       22.0|  1.0|    0.5|       3.5|         0.0|                  0.3|        29.8|                 2.5|        0.0|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "| 2023-01-01 00:31:23|  2023-01-01 00:43:25|            1.0|         3.06|       1.0|         239|          41|           1|       15.6|  1.0|    0.5|      4.12|         0.0|                  1.0|       24.72|                 2.5|        0.0|\n",
      "| 2023-01-01 00:05:11|  2023-01-01 00:37:58|            1.0|         3.62|       1.0|         142|         170|           1|       28.9|  1.0|    0.5|      6.78|         0.0|                  1.0|       40.68|                 2.5|        0.0|\n",
      "| 2023-01-01 00:03:33|  2023-01-01 00:12:57|            6.0|         1.19|       1.0|          68|         158|           1|       10.0|  1.0|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|\n",
      "| 2023-01-01 00:01:43|  2023-01-01 00:01:46|            1.0|         0.16|       5.0|         169|         169|           1|       12.0|  0.0|    0.5|       0.0|         0.0|                  1.0|        13.5|                 0.0|        0.0|\n",
      "| 2023-01-01 00:00:51|  2023-01-01 00:10:05|            1.0|         0.73|       1.0|          68|         164|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Sort the sdf by datetime so we can see if there are any trips taken before 2022-07-01 00:00:00 or after 2022-12-31 23:59:59\n",
    "yellow_sdf = yellow_sdf.sort(\"tpep_pickup_datetime\")\n",
    "yellow_sdf.show(5)\n",
    "\n",
    "yellow_sdf = yellow_sdf.sort(F.desc(\"tpep_pickup_datetime\"))\n",
    "yellow_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output we can see we do need to filter the dataframe by the afformentioned dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter by date, the amount of passengers, the fare amount and trip distance. \n",
    "yellow_sdf = yellow_sdf.filter(F.col(\"tpep_pickup_datetime\")>\"2022-07-01 00:00:00\")\\\n",
    "                       .filter(F.col(\"tpep_pickup_datetime\")<\"2022-12-31 23:59:59\")\\\n",
    "                       .filter(F.col(\"passenger_count\")>0)\\\n",
    "                       .filter(F.col(\"fare_amount\")>2.5)\\\n",
    "                       .filter(F.col(\"trip_distance\")>=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_stats (sdf, id_col):\n",
    "    \"\"\"Function coagulates the mean and standard deviation of \n",
    "    the input columns id_col and returns these in a dictionary\"\"\"\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for c in id_col:\n",
    "        stats[c] = sdf.select(F.mean(c), F.stddev(c))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trip_distance': +------------------+--------------------------+\n",
       " |avg(trip_distance)|stddev_samp(trip_distance)|\n",
       " +------------------+--------------------------+\n",
       " | 3.725231711582681|         56.77742080177889|\n",
       " +------------------+--------------------------+,\n",
       " 'total_amount': +-----------------+-------------------------+\n",
       " |avg(total_amount)|stddev_samp(total_amount)|\n",
       " +-----------------+-------------------------+\n",
       " | 22.3164832469607|       17.760110677748635|\n",
       " +-----------------+-------------------------+}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In order to identify outliers amongst the data, find the mean and standard deviation of trip distance and total amount\n",
    "stats_columns = ['trip_distance', 'total_amount']\n",
    "stats = get_sdf_stats(yellow_sdf, stats_columns)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Use these statistics to identify the 99th percentile, assuming a normal distribution,  as our maximum and filter any instancers with values greater than these\n",
    "trip_mean = stats['trip_distance'].head()[0]\n",
    "trip_stddev = stats['trip_distance'].head()[1]\n",
    "\n",
    "amount_mean = stats['total_amount'].head()[0]\n",
    "amount_stddev = stats['total_amount'].head()[1]\n",
    "\n",
    "trip_distance_limit = trip_mean + sps.norm.ppf(0.99)*trip_stddev\n",
    "total_amount_limit = amount_mean +sps.norm.ppf(0.99)*amount_stddev\n",
    "\n",
    "yellow_sdf = yellow_sdf.filter(F.col(\"total_amount\")<total_amount_limit)\\\n",
    "                       .filter(F.col(\"trip_distance\")<trip_distance_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have use total amount to remove any outliers that have been tabulated from tips, fare amount, etc. We can drop it as we will not be using it in our future analysis.\n",
    "#And since Spark.ML cannot handle NULL types we will remove them all as well\n",
    "\n",
    "yellow_clean_sdf = yellow_sdf.drop(\"total_amount\")\\\n",
    "                       .dropna('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances after filtering: 17352747\n",
      "Number of instances removed: 2485768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Now we find out ho many instances were droped from the orginal data\n",
    "\n",
    "yellow_count_post = yellow_clean_sdf.count()\n",
    "pre_post_diff = yellow_count_pre - yellow_count_post\n",
    "\n",
    "print(\"Number of instances after filtering:\", yellow_count_post)\n",
    "print(\"Number of instances removed:\", pre_post_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save our data to be used for feature engeineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/19 20:18:40 WARN MemoryManager: Total allocation exceeds 95.00% (914,961,192 bytes) of heap memory\n",
      "Scaling row group sizes to 97.39% for 7 writers\n",
      "23/08/19 20:18:40 WARN MemoryManager: Total allocation exceeds 95.00% (914,961,192 bytes) of heap memory\n",
      "Scaling row group sizes to 85.21% for 8 writers\n",
      "23/08/19 20:18:52 WARN MemoryManager: Total allocation exceeds 95.00% (914,961,192 bytes) of heap memory\n",
      "Scaling row group sizes to 97.39% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_clean_sdf.write.mode('overwrite').parquet('../data/landing/yellow_tlc_clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
